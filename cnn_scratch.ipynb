{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this project is to attempt to classify the variables malware samples, with the data taken from here https://github.com/marcoramilli/MalwareTrainingSets, using a convolutional neural network\n",
    "\n",
    "I am starting off by copying one of the samples from this repo https://github.com/aymericdamien/TensorFlow-Examples, and modifying it to fit my needs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "\n",
    "features_len = 144 #number of potential features provided, some rarely used\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/train/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division, print_function, absolute_import\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "num_steps = 3000\n",
    "batch_size = 5\n",
    "display_step = 10\n",
    "\n",
    "\n",
    "num_input = features_len\n",
    "num_classes = 30#for now\n",
    "dropout = 0.25 #tuneable\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(tf.float32, [None, num_input])\n",
    "Y = tf.placeholder(tf.float32, [None, num_classes])\n",
    "keep_prob = tf.placeholder(tf.float32) # dropout (keep probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-4-c3d55fec490c>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /home/train/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /home/train/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /home/train/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/train/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/train/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist.train.next_batch(10) #understanding data format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist.train.next_batch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxpool2d(batch_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-4-962b54b89659>:67: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#taking CNN code from https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/3_NeuralNetworks/convolutional_network_raw.ipynb,\n",
    "#working on modifying\n",
    "\n",
    "# Create some wrappers for simplicity\n",
    "def conv2d(x, W, b, strides=1):\n",
    "    # Conv2D wrapper, with bias and relu activation\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "\n",
    "def maxpool2d(x, k=2):\n",
    "    # MaxPool2D wrapper\n",
    "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1],\n",
    "                          padding='SAME')\n",
    "\n",
    "\n",
    "def conv_net(x, weights, biases, dropout):\n",
    "    try:\n",
    "        x = tf.reshape(x, shape=[-1, 12, 12, 1])\n",
    "    # Convolution Layer\n",
    "        conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
    "    # Max Pooling (down-sampling)\n",
    "        conv1 = maxpool2d(conv1, k=2)\n",
    "    # Convolution Layer\n",
    "        conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
    "    # Max Pooling (down-sampling)\n",
    "        conv2 = maxpool2d(conv2, k=2)\n",
    "    # Fully connected layer\n",
    "    # Reshape conv2 output to fit fully connected layer input\n",
    "        fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "        fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
    "        fc1 = tf.nn.relu(fc1)\n",
    "        # Apply Dropout\n",
    "        fc1 = tf.nn.dropout(fc1, dropout)\n",
    "        # Output, class prediction\n",
    "        out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "        return out    \n",
    "    except Exception as e:\n",
    "        return e\n",
    "\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    # 5x5 conv, 1 input, 32 outputs\n",
    "    'wc1': tf.Variable(tf.random_normal([5, 5, 1, 32])),\n",
    "    # 5x5 conv, 32 inputs, 64 outputs\n",
    "    'wc2': tf.Variable(tf.random_normal([5, 5, 32, 64])),\n",
    "    # fully connected, 7*7*64 inputs, 1024 outputs\n",
    "    'wd1': tf.Variable(tf.random_normal([6*6*80, 1024])),\n",
    "    # 1024 inputs, 10 outputs (class prediction)\n",
    "    'out': tf.Variable(tf.random_normal([1024, num_classes]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'bc1': tf.Variable(tf.random_normal([32])),\n",
    "    'bc2': tf.Variable(tf.random_normal([64])),\n",
    "    'bd1': tf.Variable(tf.random_normal([1024])),\n",
    "    'out': tf.Variable(tf.random_normal([num_classes]))\n",
    "}\n",
    "\n",
    "# Construct model\n",
    "logits = conv_net(X, weights, biases, keep_prob)\n",
    "prediction = tf.nn.softmax(logits)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=logits, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "\n",
    "# Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=logits, labels=Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Operation 'Adam_4' type=NoOp>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.minimize(loss_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Mean_8:0' shape=() dtype=float32>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=tf.reshape(batch_x, shape=[-1, 12, 12, 1])\n",
    "conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
    "conv2 = maxpool2d(conv1, k=2)\n",
    "conv3 = conv2d(conv2, weights['wc2'], biases['bc2'])\n",
    "conv4 = maxpool2d(conv3, k=2)\n",
    "fc1 = tf.reshape(conv4, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "fc2 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
    "fc3 = tf.nn.relu(fc2)\n",
    "    # Apply Dropout\n",
    "fc4 = tf.nn.dropout(fc3, dropout)\n",
    "    # Output, class prediction\n",
    "out = tf.add(tf.matmul(fc4, weights['out']), biases['out'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable_7:0' shape=(30,) dtype=float32_ref>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biases['out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Reshape_2:0' shape=(5, 12, 12, 1) dtype=float32>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Relu_3:0' shape=(5, 12, 12, 32) dtype=float32>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'MaxPool_2:0' shape=(5, 6, 6, 32) dtype=float32>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Relu_4:0' shape=(5, 6, 6, 64) dtype=float32>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'MaxPool_3:0' shape=(5, 3, 3, 64) dtype=float32>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Reshape_3:0' shape=(1, 2880) dtype=float32>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Add_2:0' shape=(1, 1024) dtype=float32>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Relu_5:0' shape=(1, 1024) dtype=float32>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'dropout_1/mul:0' shape=(1, 1024) dtype=float32>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Add_3:0' shape=(1, 30) dtype=float32>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Add_1:0' shape=(?, 30) dtype=float32>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 30)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 144)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zeus\n",
      "shadowbrokers\n",
      "Crypto\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "labels = np.loadtxt('labels_full.txt', dtype='str').tolist()\n",
    "\n",
    "counts = {}#dict.fromkeys(s, 0)\n",
    "for x in set(labels):\n",
    "    counts[x] = labels.count(x)\n",
    "    \n",
    "for key, value in counts.items():\n",
    "    if value > 1000:\n",
    "        print(key)\n",
    "#these are the samples that we will be classifying, since that have the bare minimum number of data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      "Step 0, Minibatch Loss= 23740676.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 10, Minibatch Loss= 7356332.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 20, Minibatch Loss= 21802004.0000, Training Accuracy= 0.200\n",
      "x\n",
      "Step 30, Minibatch Loss= 13028467.0000, Training Accuracy= 0.200\n",
      "x\n",
      "Step 40, Minibatch Loss= 20807158.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 50, Minibatch Loss= 10127242.0000, Training Accuracy= 0.200\n",
      "x\n",
      "Step 60, Minibatch Loss= 11835459.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 70, Minibatch Loss= 29465120.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 80, Minibatch Loss= 77052144.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 90, Minibatch Loss= 45644632.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 100, Minibatch Loss= 37097264.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 110, Minibatch Loss= 22928652.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 120, Minibatch Loss= 5171448.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 130, Minibatch Loss= 10661274.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 140, Minibatch Loss= 10615615.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 150, Minibatch Loss= 16544915.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 160, Minibatch Loss= 40412316.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 170, Minibatch Loss= 70181336.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 180, Minibatch Loss= 11881310.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 190, Minibatch Loss= 5720986.5000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 200, Minibatch Loss= 44983856.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 210, Minibatch Loss= 13586592.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 220, Minibatch Loss= 13057650.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 230, Minibatch Loss= 7997576.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 240, Minibatch Loss= 14382646.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 250, Minibatch Loss= 52767892.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 260, Minibatch Loss= 279820768.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 270, Minibatch Loss= 25073896.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 280, Minibatch Loss= 22368888.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 290, Minibatch Loss= 46023408.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 300, Minibatch Loss= 8014462.5000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 310, Minibatch Loss= 11834058.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 320, Minibatch Loss= 7783565.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 330, Minibatch Loss= 1914730.2500, Training Accuracy= 0.000\n",
      "x\n",
      "Step 340, Minibatch Loss= 7834840.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 350, Minibatch Loss= 8765120.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 360, Minibatch Loss= 14788018.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 370, Minibatch Loss= 4592176.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 380, Minibatch Loss= 14035998.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 390, Minibatch Loss= 10799418.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 400, Minibatch Loss= 15054730.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 410, Minibatch Loss= 3887250.5000, Training Accuracy= 0.200\n",
      "x\n",
      "Step 420, Minibatch Loss= 17366894.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 430, Minibatch Loss= 5186934.5000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 440, Minibatch Loss= 68332664.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 450, Minibatch Loss= 24197162.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 460, Minibatch Loss= 6975710.5000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 470, Minibatch Loss= 4337068.5000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 480, Minibatch Loss= 7779296.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 490, Minibatch Loss= 9463185.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 500, Minibatch Loss= 30016858.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 510, Minibatch Loss= 5654903.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 520, Minibatch Loss= 22289136.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 530, Minibatch Loss= 2612375.5000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 540, Minibatch Loss= 34621272.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 550, Minibatch Loss= 14399202.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 560, Minibatch Loss= 15714963.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 570, Minibatch Loss= 51294184.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 580, Minibatch Loss= 2360804.5000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 590, Minibatch Loss= 4060387.2500, Training Accuracy= 0.000\n",
      "x\n",
      "Step 600, Minibatch Loss= 11354934.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 610, Minibatch Loss= 6909693.5000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 620, Minibatch Loss= 71994280.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 630, Minibatch Loss= 9033354.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 640, Minibatch Loss= 8131877.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 650, Minibatch Loss= 7767337.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 660, Minibatch Loss= 6688923.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 670, Minibatch Loss= 4419634.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 680, Minibatch Loss= 16984012.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 690, Minibatch Loss= 11548100.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 700, Minibatch Loss= 2981312.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 710, Minibatch Loss= 80923840.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 720, Minibatch Loss= 25783512.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 730, Minibatch Loss= 8393474.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 740, Minibatch Loss= 16029277.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 750, Minibatch Loss= 28953168.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 760, Minibatch Loss= 5322359.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 770, Minibatch Loss= 69040176.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 780, Minibatch Loss= 6585498.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 790, Minibatch Loss= 10066510.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 800, Minibatch Loss= 11700146.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 810, Minibatch Loss= 5612462.5000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 820, Minibatch Loss= 64214228.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 830, Minibatch Loss= 1127664.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 840, Minibatch Loss= 30800084.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 850, Minibatch Loss= 44155804.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 860, Minibatch Loss= 11552858.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 870, Minibatch Loss= 19942936.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 880, Minibatch Loss= 8426778.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 890, Minibatch Loss= 30835162.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 900, Minibatch Loss= 9449730.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 910, Minibatch Loss= 5574884.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 920, Minibatch Loss= 15153965.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 930, Minibatch Loss= 4687105.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 940, Minibatch Loss= 9291738.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 950, Minibatch Loss= 18089900.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 960, Minibatch Loss= 7981430.5000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 970, Minibatch Loss= 10084300.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 980, Minibatch Loss= 22115320.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 990, Minibatch Loss= 13546704.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 1000, Minibatch Loss= 5157287.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 1010, Minibatch Loss= 49514668.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 1020, Minibatch Loss= 7410273.5000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 1030, Minibatch Loss= 68735264.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 1040, Minibatch Loss= 6937120.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 1050, Minibatch Loss= 4955887.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 1060, Minibatch Loss= 9150067.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 1070, Minibatch Loss= 6111761.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 1080, Minibatch Loss= 4302872.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 1090, Minibatch Loss= 23714612.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 1100, Minibatch Loss= 12805694.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 1110, Minibatch Loss= 13198472.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 1120, Minibatch Loss= 11364666.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 1130, Minibatch Loss= 11299501.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 1140, Minibatch Loss= 2589553.2500, Training Accuracy= 0.000\n",
      "x\n",
      "Step 1150, Minibatch Loss= 6570934.5000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 1160, Minibatch Loss= 5405049.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 1170, Minibatch Loss= 211192848.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 1180, Minibatch Loss= 22218718.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 1190, Minibatch Loss= 7719318.5000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 1200, Minibatch Loss= 12612820.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 1210, Minibatch Loss= 36525104.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 1220, Minibatch Loss= 22301372.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 1230, Minibatch Loss= 17333978.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 1240, Minibatch Loss= 5631509.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 1250, Minibatch Loss= 11386579.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 1260, Minibatch Loss= 4851387.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 1270, Minibatch Loss= 20655622.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 1280, Minibatch Loss= 6609934.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 1290, Minibatch Loss= 14078373.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 1300, Minibatch Loss= 3937640.7500, Training Accuracy= 0.000\n",
      "x\n",
      "Step 1310, Minibatch Loss= 27641276.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 1320, Minibatch Loss= 72851800.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 1330, Minibatch Loss= 155496240.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 1340, Minibatch Loss= 58691992.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 1350, Minibatch Loss= 30763482.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 1360, Minibatch Loss= 7181734.5000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 1370, Minibatch Loss= 44551840.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 1380, Minibatch Loss= 13939896.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 1390, Minibatch Loss= 10966814.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 1400, Minibatch Loss= 11303967.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 1410, Minibatch Loss= 7430073.5000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 1420, Minibatch Loss= 6094402.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 1430, Minibatch Loss= 5337966.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 1440, Minibatch Loss= 40350028.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 1450, Minibatch Loss= 16368286.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 1460, Minibatch Loss= 28253540.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 1470, Minibatch Loss= 3815828.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 1480, Minibatch Loss= 10175858.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 1490, Minibatch Loss= 171483072.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 1500, Minibatch Loss= 6347209.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 1510, Minibatch Loss= 4676622.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 1520, Minibatch Loss= 4758846.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 1530, Minibatch Loss= 5356129.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 1540, Minibatch Loss= 39107396.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 1550, Minibatch Loss= 29595404.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 1560, Minibatch Loss= 108474536.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 1570, Minibatch Loss= 20540552.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 1580, Minibatch Loss= 12453865.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 1590, Minibatch Loss= 17097936.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 1600, Minibatch Loss= 14802998.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 1610, Minibatch Loss= 80061808.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 1620, Minibatch Loss= 16457914.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 1630, Minibatch Loss= 12825575.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 1640, Minibatch Loss= 7731241.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 1650, Minibatch Loss= 6847110.5000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 1660, Minibatch Loss= 16397075.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 1670, Minibatch Loss= 2426648.7500, Training Accuracy= 0.000\n",
      "x\n",
      "Step 1680, Minibatch Loss= 12613987.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 1690, Minibatch Loss= 13592702.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 1700, Minibatch Loss= 13452026.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 1710, Minibatch Loss= 13962258.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 1720, Minibatch Loss= 6922711.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 1730, Minibatch Loss= 5003851.5000, Training Accuracy= 0.000\n",
      "x\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1740, Minibatch Loss= 17258552.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 1750, Minibatch Loss= 22174526.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 1760, Minibatch Loss= 9383479.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 1770, Minibatch Loss= 25700578.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 1780, Minibatch Loss= 12781875.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 1790, Minibatch Loss= 18809796.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 1800, Minibatch Loss= 77657600.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 1810, Minibatch Loss= 11017560.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 1820, Minibatch Loss= 20362218.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 1830, Minibatch Loss= 10253690.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 1840, Minibatch Loss= 7454553.5000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 1850, Minibatch Loss= 6119314.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 1860, Minibatch Loss= 4059997.5000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 1870, Minibatch Loss= 4377118.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 1880, Minibatch Loss= 11617838.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 1890, Minibatch Loss= 54614624.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 1900, Minibatch Loss= 1742734.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 1910, Minibatch Loss= 21939276.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 1920, Minibatch Loss= 2594619.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 1930, Minibatch Loss= 120298584.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 1940, Minibatch Loss= 73903728.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 1950, Minibatch Loss= 31552262.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 1960, Minibatch Loss= 3740719.2500, Training Accuracy= 0.000\n",
      "x\n",
      "Step 1970, Minibatch Loss= 7177895.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 1980, Minibatch Loss= 5959605.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 1990, Minibatch Loss= 9898546.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 2000, Minibatch Loss= 9398548.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 2010, Minibatch Loss= 63223736.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 2020, Minibatch Loss= 8037348.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 2030, Minibatch Loss= 3826452.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 2040, Minibatch Loss= 11114730.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 2050, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "x\n",
      "Step 2060, Minibatch Loss= 3559959.2500, Training Accuracy= 0.000\n",
      "x\n",
      "Step 2070, Minibatch Loss= 5200503.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 2080, Minibatch Loss= 18237126.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 2090, Minibatch Loss= 8955160.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 2100, Minibatch Loss= 21447014.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 2110, Minibatch Loss= 8319613.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 2120, Minibatch Loss= 21528756.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 2130, Minibatch Loss= 55478472.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 2140, Minibatch Loss= 15463163.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 2150, Minibatch Loss= 28023308.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 2160, Minibatch Loss= 7886081.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 2170, Minibatch Loss= 41602744.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 2180, Minibatch Loss= 19844604.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 2190, Minibatch Loss= 10807981.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 2200, Minibatch Loss= 102005096.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 2210, Minibatch Loss= 3521444.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 2220, Minibatch Loss= 10104007.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 2230, Minibatch Loss= 2938064.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 2240, Minibatch Loss= 32425020.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 2250, Minibatch Loss= 3863614.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 2260, Minibatch Loss= 31486064.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 2270, Minibatch Loss= 29826730.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 2280, Minibatch Loss= 25826686.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 2290, Minibatch Loss= 7389006.5000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 2300, Minibatch Loss= 12785176.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 2310, Minibatch Loss= 6514112.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 2320, Minibatch Loss= 13453138.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 2330, Minibatch Loss= 163706400.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 2340, Minibatch Loss= 68748064.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 2350, Minibatch Loss= 12883226.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 2360, Minibatch Loss= 7917059.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 2370, Minibatch Loss= 22835160.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 2380, Minibatch Loss= 32492166.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 2390, Minibatch Loss= 8121215.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 2400, Minibatch Loss= 5933393.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 2410, Minibatch Loss= 10611490.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 2420, Minibatch Loss= 6717441.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 2430, Minibatch Loss= 16962820.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 2440, Minibatch Loss= 13949014.0000, Training Accuracy= 0.200\n",
      "x\n",
      "Step 2450, Minibatch Loss= 16299328.0000, Training Accuracy= 0.000\n",
      "x\n",
      "Step 2460, Minibatch Loss= 5274698.5000, Training Accuracy= 0.200\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "feats = np.loadtxt('features.txt', dtype='float32')\n",
    "labels = np.loadtxt('onehot-labels.txt')#, dtype='float32')\n",
    "#break into test and training\n",
    "\n",
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "\n",
    "    for c in range(0,len(feats), batch_size):\n",
    "        batch_x = feats[c:c+batch_size]\n",
    "        batch_y = labels[c:c+batch_size]\n",
    "        #sess.run(logits, feed_dict={X: batch_x, Y: batch_y, keep_prob: dropout})\n",
    "        #sess.run(loss_op, feed_dict={X: batch_x, Y: batch_y, keep_prob: dropout})\n",
    "        #sess.run(train_op, feed_dict={X: batch_x, Y: batch_y, keep_prob: dropout}) ##train_op not compiling for some reason, need to investigate why\n",
    "        if c % display_step == 0 or c == 1:\n",
    "            # Calculate batch loss and accuracy\n",
    "            print('x')\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: batch_x, Y: batch_y, keep_prob: 1.0})\n",
    "            print(\"Step \" + str(c) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.3f}\".format(acc))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Calculate accuracy\n",
    "    ##pull some test data\n",
    "    #X_test,y_test = parse_mal_json(pathfiles)\n",
    "    \n",
    "    #print(\"Testing Accuracy:\", \\\n",
    "     #   sess.run(accuracy, feed_dict={X: ,\n",
    "     #                                 Y: ,\n",
    "     #                                 keep_prob: 1.0}))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Input to reshape is a tensor with 150 values, but the requested shape has 30\n\t [[Node: gradients/softmax_cross_entropy_with_logits_sg/Reshape_grad/Reshape = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](gradients/softmax_cross_entropy_with_logits_sg_grad/mul, softmax_cross_entropy_with_logits_sg/Shape)]]\n\nCaused by op 'gradients/softmax_cross_entropy_with_logits_sg/Reshape_grad/Reshape', defined at:\n  File \"/home/train/anaconda3/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/train/anaconda3/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/train/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/train/anaconda3/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/train/anaconda3/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/home/train/anaconda3/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 127, in start\n    self.asyncio_loop.run_forever()\n  File \"/home/train/anaconda3/lib/python3.6/asyncio/base_events.py\", line 422, in run_forever\n    self._run_once()\n  File \"/home/train/anaconda3/lib/python3.6/asyncio/base_events.py\", line 1432, in _run_once\n    handle._run()\n  File \"/home/train/anaconda3/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/home/train/anaconda3/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 117, in _handle_events\n    handler_func(fileobj, events)\n  File \"/home/train/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/train/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/home/train/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/train/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/train/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/train/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/train/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/train/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/train/anaconda3/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/train/anaconda3/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/train/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/train/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/train/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2903, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/train/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-4-962b54b89659>\", line 69, in <module>\n    train_op = optimizer.minimize(loss_op)\n  File \"/home/train/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\", line 414, in minimize\n    grad_loss=grad_loss)\n  File \"/home/train/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\", line 526, in compute_gradients\n    colocate_gradients_with_ops=colocate_gradients_with_ops)\n  File \"/home/train/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 494, in gradients\n    gate_gradients, aggregation_method, stop_gradients)\n  File \"/home/train/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 636, in _GradientsHelper\n    lambda: grad_fn(op, *out_grads))\n  File \"/home/train/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 385, in _MaybeCompile\n    return grad_fn()  # Exit early\n  File \"/home/train/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 636, in <lambda>\n    lambda: grad_fn(op, *out_grads))\n  File \"/home/train/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/array_grad.py\", line 521, in _ReshapeGrad\n    return [array_ops.reshape(grad, array_ops.shape(op.inputs[0])), None]\n  File \"/home/train/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 6113, in reshape\n    \"Reshape\", tensor=tensor, shape=shape, name=name)\n  File \"/home/train/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/train/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3392, in create_op\n    op_def=op_def)\n  File \"/home/train/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1718, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\n...which was originally created as op 'softmax_cross_entropy_with_logits_sg/Reshape', defined at:\n  File \"/home/train/anaconda3/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n[elided 22 identical lines from previous traceback]\n  File \"/home/train/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-4-962b54b89659>\", line 67, in <module>\n    logits=logits, labels=Y))\n  File \"/home/train/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 250, in new_func\n    return func(*args, **kwargs)\n  File \"/home/train/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 1959, in softmax_cross_entropy_with_logits\n    labels=labels, logits=logits, dim=dim, name=name)\n  File \"/home/train/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 1866, in softmax_cross_entropy_with_logits_v2\n    precise_logits = _flatten_outer_dims(precise_logits)\n  File \"/home/train/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 1612, in _flatten_outer_dims\n    output = array_ops.reshape(logits, array_ops.concat([[-1], last_dim_size], 0))\n  File \"/home/train/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 6113, in reshape\n    \"Reshape\", tensor=tensor, shape=shape, name=name)\n  File \"/home/train/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/train/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3392, in create_op\n    op_def=op_def)\n  File \"/home/train/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1718, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): Input to reshape is a tensor with 150 values, but the requested shape has 30\n\t [[Node: gradients/softmax_cross_entropy_with_logits_sg/Reshape_grad/Reshape = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](gradients/softmax_cross_entropy_with_logits_sg_grad/mul, softmax_cross_entropy_with_logits_sg/Shape)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Input to reshape is a tensor with 150 values, but the requested shape has 30\n\t [[Node: gradients/softmax_cross_entropy_with_logits_sg/Reshape_grad/Reshape = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](gradients/softmax_cross_entropy_with_logits_sg_grad/mul, softmax_cross_entropy_with_logits_sg/Shape)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-c4f7a3839cd7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m#sess.run(logits, feed_dict={X: batch_x, Y: batch_y, keep_prob: dropout})\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m#sess.run(loss_op, feed_dict={X: batch_x, Y: batch_y, keep_prob: dropout})\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m##train_op not compiling for some reason, need to investigate why\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdisplay_step\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;31m# Calculate batch loss and accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1333\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1334\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1335\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1337\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Input to reshape is a tensor with 150 values, but the requested shape has 30\n\t [[Node: gradients/softmax_cross_entropy_with_logits_sg/Reshape_grad/Reshape = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](gradients/softmax_cross_entropy_with_logits_sg_grad/mul, softmax_cross_entropy_with_logits_sg/Shape)]]\n\nCaused by op 'gradients/softmax_cross_entropy_with_logits_sg/Reshape_grad/Reshape', defined at:\n  File \"/home/train/anaconda3/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/train/anaconda3/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/train/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/train/anaconda3/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/train/anaconda3/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/home/train/anaconda3/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 127, in start\n    self.asyncio_loop.run_forever()\n  File \"/home/train/anaconda3/lib/python3.6/asyncio/base_events.py\", line 422, in run_forever\n    self._run_once()\n  File \"/home/train/anaconda3/lib/python3.6/asyncio/base_events.py\", line 1432, in _run_once\n    handle._run()\n  File \"/home/train/anaconda3/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/home/train/anaconda3/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 117, in _handle_events\n    handler_func(fileobj, events)\n  File \"/home/train/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/train/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/home/train/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/train/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/train/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/train/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/train/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/train/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/train/anaconda3/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/train/anaconda3/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/train/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/train/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/train/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2903, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/train/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-4-962b54b89659>\", line 69, in <module>\n    train_op = optimizer.minimize(loss_op)\n  File \"/home/train/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\", line 414, in minimize\n    grad_loss=grad_loss)\n  File \"/home/train/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\", line 526, in compute_gradients\n    colocate_gradients_with_ops=colocate_gradients_with_ops)\n  File \"/home/train/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 494, in gradients\n    gate_gradients, aggregation_method, stop_gradients)\n  File \"/home/train/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 636, in _GradientsHelper\n    lambda: grad_fn(op, *out_grads))\n  File \"/home/train/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 385, in _MaybeCompile\n    return grad_fn()  # Exit early\n  File \"/home/train/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 636, in <lambda>\n    lambda: grad_fn(op, *out_grads))\n  File \"/home/train/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/array_grad.py\", line 521, in _ReshapeGrad\n    return [array_ops.reshape(grad, array_ops.shape(op.inputs[0])), None]\n  File \"/home/train/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 6113, in reshape\n    \"Reshape\", tensor=tensor, shape=shape, name=name)\n  File \"/home/train/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/train/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3392, in create_op\n    op_def=op_def)\n  File \"/home/train/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1718, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\n...which was originally created as op 'softmax_cross_entropy_with_logits_sg/Reshape', defined at:\n  File \"/home/train/anaconda3/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n[elided 22 identical lines from previous traceback]\n  File \"/home/train/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-4-962b54b89659>\", line 67, in <module>\n    logits=logits, labels=Y))\n  File \"/home/train/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 250, in new_func\n    return func(*args, **kwargs)\n  File \"/home/train/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 1959, in softmax_cross_entropy_with_logits\n    labels=labels, logits=logits, dim=dim, name=name)\n  File \"/home/train/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 1866, in softmax_cross_entropy_with_logits_v2\n    precise_logits = _flatten_outer_dims(precise_logits)\n  File \"/home/train/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 1612, in _flatten_outer_dims\n    output = array_ops.reshape(logits, array_ops.concat([[-1], last_dim_size], 0))\n  File \"/home/train/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 6113, in reshape\n    \"Reshape\", tensor=tensor, shape=shape, name=name)\n  File \"/home/train/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/train/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3392, in create_op\n    op_def=op_def)\n  File \"/home/train/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1718, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): Input to reshape is a tensor with 150 values, but the requested shape has 30\n\t [[Node: gradients/softmax_cross_entropy_with_logits_sg/Reshape_grad/Reshape = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](gradients/softmax_cross_entropy_with_logits_sg_grad/mul, softmax_cross_entropy_with_logits_sg/Shape)]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "feats = np.loadtxt('features.txt', dtype='float32')\n",
    "labels = np.loadtxt('onehot-labels.txt')#, dtype='float32')\n",
    "#break into test and training\n",
    "\n",
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "\n",
    "    for c in range(0,len(feats), batch_size):\n",
    "        batch_x = feats[c:c+batch_size]\n",
    "        batch_y = labels[c:c+batch_size]\n",
    "        #sess.run(logits, feed_dict={X: batch_x, Y: batch_y, keep_prob: dropout})\n",
    "        #sess.run(loss_op, feed_dict={X: batch_x, Y: batch_y, keep_prob: dropout})\n",
    "        sess.run(train_op, feed_dict={X: batch_x, Y: batch_y, keep_prob: dropout}) ##train_op not compiling for some reason, need to investigate why\n",
    "        if c % display_step == 0 or c == 1:\n",
    "            # Calculate batch loss and accuracy\n",
    "            print('x')\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: batch_x, Y: batch_y, keep_prob: 1.0})\n",
    "            print(\"Step \" + str(c) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.3f}\".format(acc))\n",
    "\n",
    "    print(\"Optimization Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
