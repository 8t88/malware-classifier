{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook works through the malware samples folder and parses out the json to make feature vectors.\n",
    "The explanation of the data can be found here https://marcoramilli.blogspot.com/2016/12/malware-training-sets-machine-learning.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "\n",
    "sample_path = 'MalwareTrainingSets-master/trainingSets/Careto_Feb2014-1201117/1201117_dmyql.json'\n",
    "\n",
    "sample_json_str = open(sample_path).read()\n",
    "sample_json_dict = json.loads(sample_json_str)\n",
    "\n",
    "#get a list of the files\n",
    "base = 'MalwareTrainingSets-master/trainingSets'\n",
    "\n",
    "#adding a condition to check that its a json file, because there is at least one '.DS_Store' file in there\n",
    "mal_paths = [os.path.join(dirName, file) for dirName, subdirList, fileList in os.walk(base) for file in fileList if 'json' in file]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##get all possible field names\n",
    "feature_dict = {}\n",
    "for filepath in mal_paths:\n",
    "    json_str = open(filepath).read()\n",
    "    json_dict = json.loads(json_str)\n",
    "    \n",
    "    for key,_ in json_dict['properties'].items():\n",
    "        feature_dict[key] = 0\n",
    "\n",
    "#maybe make more efficient & feature-proof later\n",
    "#np.savetxt('feature_names.txt', feature_names, fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to parse the data\n",
    "def parse_mal_json(filepath):\n",
    "    local_dict = dict(feature_dict)\n",
    "    json_str = open(filepath).read()\n",
    "    json_dict = json.loads(json_str)\n",
    "    \n",
    "    for key,value in json_dict['properties'].items():\n",
    "        local_dict[key] = len(value.split())##other thought, see how many examples in each entry, and 0 if no entry\n",
    "\n",
    "        \n",
    "    del local_dict['label']\n",
    "    label_name = json_dict['properties']['label'].split('.')[0].split('-')[0]\n",
    "    \n",
    "    #one-hot-encode label, actually maybe after\n",
    "    return (np.array(list(local_dict.values()), dtype='float32'), label_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterate through\n",
    "(features, label) = parse_mal_json(mal_paths[0])\n",
    "for path in mal_paths[1:]:\n",
    "    try:\n",
    "        tup = parse_mal_json(path)\n",
    "        features = np.vstack((features, tup[0]))\n",
    "        label = np.vstack((label, tup[1]))\n",
    "    #append to some structure\n",
    "    #save total structure to local file\n",
    "    #one-hot-encode labels\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save arrays to local file, so can access them in next notebook\n",
    "np.savetxt('features_full.txt', features, fmt='%f')\n",
    "np.savetxt('labels_full.txt', label, fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hacky for now\n",
    "pd_df = pd.DataFrame(label, columns=['label'])\n",
    "onehot = pd.get_dummies(pd_df['label'])\n",
    "\n",
    "np.savetxt('onehot-labels_full.txt', np.array(onehot), fmt='%f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing a Separate File that contains only data where from labels that have what seem like a large enough number of points; its much easier to do this here than in the actual deeplearning model code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_to_classify = []\n",
    "label_list = np.loadtxt('labels_full.txt', dtype='str').tolist()\n",
    "\n",
    "\n",
    "counts = {}\n",
    "for x in set(label_list):\n",
    "    counts[x] = label_list.count(x)\n",
    "    \n",
    "for key, value in counts.items():\n",
    "    if value > 1000:\n",
    "        labels_to_classify.append(key)\n",
    "#these are the samples that we will be classifying, since that have the bare minimum number of data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterate through\n",
    "(features_filtered, labels_filtered) = parse_mal_json(mal_paths[0]) #setting up the structure of the array\n",
    "for path in mal_paths:\n",
    "    try:\n",
    "        tup = parse_mal_json(path)\n",
    "        if(tup[1] in labels_to_classify):\n",
    "            features_filtered = np.vstack((features_filtered, tup[0]))\n",
    "            labels_filtered = np.vstack((labels_filtered, tup[1]))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(path)\n",
    "#delete the first entry, as it was just a placeholder\n",
    "features_filtered = np.delete(features_filtered, 0, 0)\n",
    "labels_filtered = np.delete(labels_filtered, 0, 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81\n"
     ]
    }
   ],
   "source": [
    "##getting rid of the features that are always zero, then padding the vectors to make the length a perfect square\n",
    "max_vec = np.amax(features_filtered, axis=0)\n",
    "zero_indexes = np.where(max_vec==0)\n",
    "\n",
    "\n",
    "shortened_features = np.delete(features_filtered, zero_indexes, axis = 1)\n",
    "features_filtered = np.pad(shortened_features, ((0,0),(0,8)), 'constant')\n",
    "print(len(features_filtered[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save arrays to local file, so can access them in next notebook\n",
    "np.savetxt('features_filtered.txt', features_filtered, fmt='%f')\n",
    "np.savetxt('labels_filtered.txt', labels_filtered, fmt='%s')\n",
    "\n",
    "#hacky for now\n",
    "pd_df = pd.DataFrame(labels_filtered, columns=['label'])\n",
    "onehot = pd.get_dummies(pd_df['label'])\n",
    "\n",
    "np.savetxt('onehot-labels_filtered.txt', np.array(onehot), fmt='%f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
